{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtzDp3XfzR3wmllRbCBZ3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hongqin/Generative_AI_Fa25/blob/main/teacher_student_mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How it works\n",
        "\n",
        "A teacher MLP is trained on MNIST (2 hidden layers).\n",
        "\n",
        "Its logits (softened by temperature T) become “soft targets.”\n",
        "\n",
        "A smaller student MLP learns from both the hard labels and the teacher’s soft targets via a combined loss.\n",
        "\n",
        "You’ll see teacher vs. student test accuracies printed at the end."
      ],
      "metadata": {
        "id": "PaxJlVIChPST"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSVr2WEFgoMa",
        "outputId": "13389f29-33c8-4109-801d-1e5a85a257ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 40.5MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.15MB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.88MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.34MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Teacher] Epoch 1/3, Loss=2.3043\n",
            "[Teacher] Epoch 2/3, Loss=2.2933\n",
            "[Teacher] Epoch 3/3, Loss=2.2783\n",
            "Teacher Test Acc: 23.27%\n",
            "\n",
            "[Student] Epoch 1/3, DistillLoss=nan\n",
            "[Student] Epoch 2/3, DistillLoss=nan\n",
            "[Student] Epoch 3/3, DistillLoss=nan\n",
            "Student Test Acc: 9.80%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Toy Teacher-Student Distillation Demo\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "# 1) Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 2) Define Teacher and Student\n",
        "class TeacherNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# 3) Hyperparameters\n",
        "batch_size      = 64\n",
        "teacher_epochs  = 3\n",
        "student_epochs  = 3\n",
        "lr              = 0.01\n",
        "T               = 2.0    # distillation temperature\n",
        "alpha           = 0.5    # mix weight between hard & soft losses\n",
        "\n",
        "# 4) Data loaders (small subset of MNIST for speed)\n",
        "transform = transforms.ToTensor()\n",
        "full_train = datasets.MNIST(root='.', train=True, download=True, transform=transform)\n",
        "train_subset = Subset(full_train, list(range(2000)))\n",
        "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = DataLoader(\n",
        "    datasets.MNIST(root='.', train=False, download=True, transform=transform),\n",
        "    batch_size=1000, shuffle=False\n",
        ")\n",
        "\n",
        "# 5) Evaluation helper\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            out = model(x)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "# 6) Train the Teacher\n",
        "teacher   = TeacherNet().to(device)\n",
        "opt_t     = optim.SGD(teacher.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, teacher_epochs+1):\n",
        "    teacher.train()\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        opt_t.zero_grad()\n",
        "        logits = teacher(x)\n",
        "        loss   = criterion(logits, y)\n",
        "        loss.backward()\n",
        "        opt_t.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Teacher] Epoch {epoch}/{teacher_epochs}, Loss={total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(f\"Teacher Test Acc: {evaluate(teacher, test_loader):.2f}%\\n\")\n",
        "\n",
        "# 7) Train the Student via Distillation\n",
        "student = StudentNet().to(device)\n",
        "opt_s    = optim.SGD(student.parameters(), lr=lr)\n",
        "\n",
        "def distill_loss(s_logits, t_logits, y, T, alpha):\n",
        "    p_t = F.log_softmax(t_logits / T, dim=1)\n",
        "    p_s = F.log_softmax(s_logits / T, dim=1)\n",
        "    loss_soft = F.kl_div(p_s, p_t, reduction='batchmean') * (T*T)\n",
        "    loss_hard = F.cross_entropy(s_logits, y)\n",
        "    return alpha * loss_hard + (1 - alpha) * loss_soft\n",
        "\n",
        "for epoch in range(1, student_epochs+1):\n",
        "    student.train()\n",
        "    total_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            t_logits = teacher(x)\n",
        "        opt_s.zero_grad()\n",
        "        s_logits = student(x)\n",
        "        loss      = distill_loss(s_logits, t_logits, y, T, alpha)\n",
        "        loss.backward()\n",
        "        opt_s.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"[Student] Epoch {epoch}/{student_epochs}, DistillLoss={total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "print(f\"Student Test Acc: {evaluate(student, test_loader):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Mn8ev2wgo8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}