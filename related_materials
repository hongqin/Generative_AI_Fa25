harvard open course
https://nlp.seas.harvard.edu/annotated-transformer/

https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/
https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits//LecturesByTags/

andrei karpathy
https://github.com/karpathy
https://github.com/karpathy/nanoGPT
https://github.com/karpathy/minGPT

Sebastian Raschka
https://github.com/rasbt

UVA, Qi
https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/
https://qiyanjun.github.io/2025sp-GenAI-overview//About/

LLM and autonomous driving
https://github.com/Thinklab-SJTU/Awesome-LLM4AD

https://bios740.github.io/syllabus/

mamba and state space model

Foundationa of large language models https://arxiv.org/abs/2501.09223

LensLLM
https://arxiv.org/abs/2505.03793


Understanding machine learning from theory to algorithms, book. 

Yilun Xu: https://yilun-xu.com/
(i) New models: PFGM [10], PFGM++ [13], HGF [18], t-EDM [21]
(ii) Training: STF [11], Disco-Diff [16], Style Control [9]
(iii) Sampling: Restart sampling [14], Particle Guidance [15], Anytime AR [5]
(iv) Discrete diffusion: DDPD [19], EDLM [20]
(v) Diffusion Distillation: TCM [22], f-distill [23]

formal reasoning meets LLM
https://www.youtube.com/live/XuKeSzc7f_c?si=oolwtlAgQPnvXxxV

organoid neuron network
https://www.cell.com/neuron/fulltext/S0896-6273(22)00806-6

virtual lab, AI co-scientists. 
https://www.nature.com/articles/d41586-025-02028-5?utm_source=chatgpt.com
the standford github repo can be run in CoLab, which can be an exercise. 

Cell learning
https://ieeexplore.ieee.org/document/9764721
https://mpinb.mpg.de/en/research-groups/groups/cellular-computations-and-learning/news-ccl-eng/koseska-synergygrant-celearn.html

Nat Biomed Eng, 2025 Jun 16. doi: 10.1038/s41551-025-01421-9. Online ahead of print.
A robust and scalable framework for hallucination detection in virtual tissue staining and digital pathology
https://pubmed.ncbi.nlm.nih.gov/40523934/

AI alignment a comprehensive survey
https://arxiv.org/abs/2310.19852

Quantifying detection rates for dangerous capabilities: a theoretical model of dangerous capability evaluations
https://arxiv.org/abs/2412.15433

Real-world deployment of a fine-tuned pathology foundation model for lung cancer biomarker detection
https://www.nature.com/articles/s41591-025-03780-x

Provable Maximum Entropy Manifold Exploration via Diffusion Models
https://arxiv.org/pdf/2506.15385

La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching
https://arxiv.org/abs/2507.09466

google's attenstion male, female, father-mother, japan - sushi, germany - beer? 
In this paper the authors show that simply by training a skip-gram model you get vector offsets that solve analogies such as

several recent works have taken the classic word-vector analogies (king–man+woman≈queen, father:mother, Japan:sushi, Germany:beer) and studied whether and how ChatGPT and its successors can do the same:

Emergent Analogical Reasoning in Large Language Models 
Webb et al. (2022) directly compare human performance to GPT-3 (text-davinci-003) and early GPT-4 on a broad suite of analogy problems (including semantic and abstract matrix analogies), showing that these LLMs already exhibit strong zero-shot analogical reasoning 
arXiv
Nature
https://www.nature.com/articles/s41562-023-01659-w?
.

ANALOGYKB: Unlocking Analogical Reasoning of Language Models with a Million-scale Knowledge Base
Yuan et al. (ACL 2024) construct a huge analogy knowledge base and demonstrate that both InstructGPT-003 and ChatGPT can leverage it to improve analogy recognition and generation (with human-level explanation accuracy on tasks like “father:mother” or “country→iconic food”) 
ACL Anthology
.

Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models
Lewis & Mitchell (2024) create “counterfactual” variants of classic analogy problems and show that, although GPT-3 and GPT-4 perform well on familiar analogies, their accuracy drops sharply on these novel cases—revealing gaps in true abstract reasoning 
arXiv
.
male : female (e.g. king – man + woman ≈ queen)

father : mother

# country → iconic food (e.g. Japan – sushi + Germany ≈ beer) 
ACL Anthology; OpenTable Technology Blog
They further explore a larger “Google analogy” test set in their follow-up work:
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). Distributed Representations of Words and Phrases and Their Compositionality. NeurIPS, 3111–3119. 
arxiv.org
several recent works have taken the classic word-vector analogies (king–man+woman≈queen, father:mother, Japan:sushi, Germany:beer) and studied whether and how ChatGPT and its successors can do the same:
Emergent Analogical Reasoning in Large Language Models
Webb et al. (2022) directly compare human performance to GPT-3 (text-davinci-003) and early GPT-4 on a broad suite of analogy problems (including semantic and abstract matrix analogies), showing that these LLMs already exhibit strong zero-shot analogical reasoning 
arXiv
Nature
ANALOGYKB: Unlocking Analogical Reasoning of Language Models with a Million-scale Knowledge Base
Yuan et al. (ACL 2024) construct a huge analogy knowledge base and demonstrate that both InstructGPT-003 and ChatGPT can leverage it to improve analogy recognition and generation (with human-level explanation accuracy on tasks like “father:mother” or “country→iconic food”) 
ACL Anthology
.
Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models
Lewis & Mitchell (2024) create “counterfactual” variants of classic analogy problems and show that, although GPT-3 and GPT-4 perform well on familiar analogies, their accuracy drops sharply on these novel cases—revealing gaps in true abstract reasoning 
arXiv
.



