stanford self improving AI agents
https://cs329a.stanford.edu/#intro

harvard open course
https://nlp.seas.harvard.edu/annotated-transformer/

https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/
https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits//LecturesByTags/



andrei karpathy
https://github.com/karpathy
https://github.com/karpathy/nanoGPT
https://github.com/karpathy/minGPT

MIT
https://introtodeeplearning.com/
https://introtodeeplearning.com/slides/6S191_MIT_DeepLearning_L4.pdf

Sebastian Raschka
https://github.com/rasbt

UVA, Qi
https://qiyanjun.github.io/2024sp-GenAI-Risk-Benefits/
https://qiyanjun.github.io/2025sp-GenAI-overview//About/

EVO model from Berkely, eric nyuen 


LLM and autonomous driving
https://github.com/Thinklab-SJTU/Awesome-LLM4AD

https://bios740.github.io/syllabus/

mamba and state space model

Foundationa of large language models https://arxiv.org/abs/2501.09223

LensLLM
https://arxiv.org/abs/2505.03793


Understanding machine learning from theory to algorithms, book. 

Yilun Xu: https://yilun-xu.com/
(i) New models: PFGM [10], PFGM++ [13], HGF [18], t-EDM [21]
(ii) Training: STF [11], Disco-Diff [16], Style Control [9]
(iii) Sampling: Restart sampling [14], Particle Guidance [15], Anytime AR [5]
(iv) Discrete diffusion: DDPD [19], EDLM [20]
(v) Diffusion Distillation: TCM [22], f-distill [23]

formal reasoning meets LLM
https://www.youtube.com/live/XuKeSzc7f_c?si=oolwtlAgQPnvXxxV

organoid neuron network
https://www.cell.com/neuron/fulltext/S0896-6273(22)00806-6

virtual lab, AI co-scientists. 
https://www.nature.com/articles/d41586-025-02028-5?utm_source=chatgpt.com
the standford github repo can be run in CoLab, which can be an exercise. 

Cell learning
https://ieeexplore.ieee.org/document/9764721
https://mpinb.mpg.de/en/research-groups/groups/cellular-computations-and-learning/news-ccl-eng/koseska-synergygrant-celearn.html

Nat Biomed Eng, 2025 Jun 16. doi: 10.1038/s41551-025-01421-9. Online ahead of print.
A robust and scalable framework for hallucination detection in virtual tissue staining and digital pathology
https://pubmed.ncbi.nlm.nih.gov/40523934/

AI alignment a comprehensive survey
https://arxiv.org/abs/2310.19852

Quantifying detection rates for dangerous capabilities: a theoretical model of dangerous capability evaluations
https://arxiv.org/abs/2412.15433

Real-world deployment of a fine-tuned pathology foundation model for lung cancer biomarker detection
https://www.nature.com/articles/s41591-025-03780-x

Provable Maximum Entropy Manifold Exploration via Diffusion Models
https://arxiv.org/pdf/2506.15385

La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching
https://arxiv.org/abs/2507.09466

google's attenstion male, female, father-mother, japan - sushi, germany - beer? 
In this paper the authors show that simply by training a skip-gram model you get vector offsets that solve analogies such as

several recent works have taken the classic word-vector analogies (king–man+woman≈queen, father:mother, Japan:sushi, Germany:beer) and studied whether and how ChatGPT and its successors can do the same:

Emergent Analogical Reasoning in Large Language Models 
Webb et al. (2022) directly compare human performance to GPT-3 (text-davinci-003) and early GPT-4 on a broad suite of analogy problems (including semantic and abstract matrix analogies), showing that these LLMs already exhibit strong zero-shot analogical reasoning 
arXiv
Nature
https://www.nature.com/articles/s41562-023-01659-w?
.

ANALOGYKB: Unlocking Analogical Reasoning of Language Models with a Million-scale Knowledge Base
Yuan et al. (ACL 2024) construct a huge analogy knowledge base and demonstrate that both InstructGPT-003 and ChatGPT can leverage it to improve analogy recognition and generation (with human-level explanation accuracy on tasks like “father:mother” or “country→iconic food”) 
ACL Anthology
.

Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models
Lewis & Mitchell (2024) create “counterfactual” variants of classic analogy problems and show that, although GPT-3 and GPT-4 perform well on familiar analogies, their accuracy drops sharply on these novel cases—revealing gaps in true abstract reasoning 
arXiv
.
male : female (e.g. king – man + woman ≈ queen)

father : mother

# country → iconic food (e.g. Japan – sushi + Germany ≈ beer) 
ACL Anthology; OpenTable Technology Blog
They further explore a larger “Google analogy” test set in their follow-up work:
Mikolov, T., Sutskever, I., Chen, K., Corrado, G., & Dean, J. (2013). Distributed Representations of Words and Phrases and Their Compositionality. NeurIPS, 3111–3119. 
arxiv.org
several recent works have taken the classic word-vector analogies (king–man+woman≈queen, father:mother, Japan:sushi, Germany:beer) and studied whether and how ChatGPT and its successors can do the same:
Emergent Analogical Reasoning in Large Language Models
Webb et al. (2022) directly compare human performance to GPT-3 (text-davinci-003) and early GPT-4 on a broad suite of analogy problems (including semantic and abstract matrix analogies), showing that these LLMs already exhibit strong zero-shot analogical reasoning 
arXiv
Nature
ANALOGYKB: Unlocking Analogical Reasoning of Language Models with a Million-scale Knowledge Base
Yuan et al. (ACL 2024) construct a huge analogy knowledge base and demonstrate that both InstructGPT-003 and ChatGPT can leverage it to improve analogy recognition and generation (with human-level explanation accuracy on tasks like “father:mother” or “country→iconic food”) 
ACL Anthology
.
Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models
Lewis & Mitchell (2024) create “counterfactual” variants of classic analogy problems and show that, although GPT-3 and GPT-4 perform well on familiar analogies, their accuracy drops sharply on these novel cases—revealing gaps in true abstract reasoning 
arXiv
.

Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct Preference Optimization
https://arxiv.org/abs/2502.13146


### ALphaChip, AI design a better AI? 

# promp injection, LLM security
https://github.com/wu4f/cyberpdx-crypto
o All material has been made publicly available (https://codelabs.cs.pdx.edu, https://github.com/wu4f/cs410g-src, https://github.com/wu4f/cyberpdx-crypto)

o YouTube playlist for Generative Security Applications course is available at (https://youtube.com/playlist?list=PLdCTMpWhMq8KuRHaXHuf2U53hM1NkuiCe&si=S5dXzkslHPX5N13w)

o CyberPDX module available at https://crypto.cyberpdx.org

video generations
https://arxiv.org/abs/2507.07202

nvidia small langauge model
https://arxiv.org/pdf/2506.02153

Major universities offer valuable slide resources and online course materials on deep generative learning and generative AI. Here are some notable sources:

- **Stanford University**: The course "CS236: Deep Generative Models" by Prof. Stefano Ermon provides detailed lecture videos and slides focused on the foundations, challenges, and applications of generative models in image, text, video, medicine, robotics, and more. The course website with slides is available at https://deepgenerativemodels.github.io/ and videos are on YouTube.[2]

- **Cornell University**: The course "CS 6785: Deep Generative Models," taught by Prof. Vadir Kuleshov, offers an introduction to deep generative models, recent advances, algorithms, and applications including NLP and biology. The lectures are recorded and available on YouTube.[4]

- **MIT (Massachusetts Institute of Technology)**: The "Introduction to Deep Learning 6.S191" program covers deep learning basics along with generative AI applications in media, vision, NLP, and biology. All lecture slides, labs, and code are open-sourced and free to use, accessible at https://introtodeeplearning.com/ with lecture videos like "Generative AI for Media" by Google’s Doug Eck on YouTube.[8][10]

- **Harvard University**: There are presentations specifically on generative AI’s role in education, including outlines for PowerPoint slides targeting its teaching and learning impact, available in PDF form (e.g., from Harvard AI Sandbox materials).[1]

- **University of Virginia (UVA)**: UVA SEAS offers collections of slides on the technical foundations of generative AI with practical uses in engineering design and analysis.[5]

- **Other resources**:
  - NVIDIA Deep Learning Institute has a teaching kit for generative AI with lecture slides, labs, and Jupyter notebooks focused on GPU-accelerated generative AI development.[6]
  - Stony Brook University provides teaching resources on generative AI including PowerPoint slides for educators.[9]

If you want ready-to-use lecture slides or full course materials, the Stanford CS236, Cornell CS6785, and MIT 6.S191 courses are among the most comprehensive and authoritative sources from major universities. Their materials are typically publicly available online for educational use.

Would you like direct access links, or specific slide decks on any of these?

[1] https://hcsra.sph.harvard.edu/sites/projects.iq.harvard.edu/files/hcsra/files/presentation_on_ai1.pdf
[2] https://www.youtube.com/watch?v=XZ0PMRWXBEU
[3] https://blog.uwgb.edu/catl/files/2023/02/Introduction-to-Generative-AI-CATL-Presentation-Slides.pdf
[4] https://www.youtube.com/watch?v=IZgvgLy1wyg
[5] https://teaching.virginia.edu/collections/uva-seas-resources-teaching-genai-use-for-engineering-design-and-analysis/272
[6] https://developer.nvidia.com/blog/nvidia-deep-learning-institute-releases-new-generative-ai-teaching-kit/
[7] https://www.sdccd.edu/docs/IIE/ProfessionalDevelopment/Presentations/10252024_AI-Demystified-Intro-to-Generative-AI.pdf
[8] https://introtodeeplearning.com
[9] https://www.stonybrook.edu/celt/teaching-resources/aibot.php
[10] https://www.youtube.com/watch?v=P7Hkh2zOGQ0

